Transformer and BERT models 
reacent year used neural netwrok to generate text , images etc . 
The major sucess was the attention machenism  , transformer and llm in 2017  .
The research papper that defined the transformer was "attention is all you need" came up in 2017 .
The moeel before transformer represented the object by vector , the contect of the words/ vector were changed accroding to the sentence and the refrance . 
Ex ->bank in river bank and bank in bank riobbery  .

What is a  transformer ?
A transformer is a encoder - decoder model that uses the attention mechansim .
it takes the advantage of paralellization  .
Attention mechansim is a adavnateg for these to do the compuation .

Basic model of a transformner ->
The transformer has 2 main unit  :
encoder - > this is the ubnit thta is used to encode th data .
decoder -> this is the unit that is used to decode the data  for a revelant task .
IN the first model that proposed the transformer there were a total of 6 endcoder and a 6 decoder unit in the model first tinme proposed .

In that paper 6 was just a hyoperparameter given for the number of decoder and tencoder unit  .

The structure of encoder -> 
self attention layer -> the self atterntion passes the input of the sentences and encode the input word .  
Feed forwad layer -> the output of the self attention layer is passed through the feedforward neural netowrk  . 
There are same work for feedforward neural network in each encoder unit . 
In the decoder layer there are self - attention  , feedforward neural network and there is also a encoder - decoder attnetuion unit . in the decoder .

Vector passes through these very smoothly with proper encoding and decoding  .
encoder - decoder attention layer focus on the main keyword and the main inoput kerywords 
There are dependencies in the self attention layer and the feedfowards neural network , 
The feedforward unit in each encoder are not dependent on each other  and multiple computation are supported in paralell  .

In the self attention layer there are multiple steps that are being executed : 
the data is first broken down into multiple unit such as 

1) query vector 
2)key vector 
3) value vector 
then there is a formation of learned weights .
The computation is done using weights that the tranforme leanrs during the traning process   / learning  .
In this the computation is in the form of matrix computation .
the formula used is : 
softma (x) {query val * (key mtrix)transpose / data value(tinyvalues)}*value vector  = z vector(matrix) .
Proper computation is done and the revelent values are taken into consideration  , rest are discarded . 

STEP-2 
now each output that u get after doing the above maths , add them up and pass them to the feed forward neural netowrk .The output of whole attention layer is passed into the systemfor the feed forward neural network . 

Overview of computation 
1-> input the sentences . 
2-> embed each word 
3-> perform multi headed atteention and multiple embedded word with the respective weighted matrices  . // calcuation part (matrix multiplication)
Calculate the attention with using the resultant matrix .

// concatenate the matrix. 
What are the different transformer ? 
some of the transfomer has only encoder and some has encoder+decoder . 
The encoder only model is BERT (bidirectional encoder representation from transfornmer)
This was developed by google in 2018 .
The bert model was trained for 1 million problem and the whole wikipedia was feeded to ther bert model .

BERT is made for multi tasking . 
Bert has 2 varients basically -> the bert base and the bert large . 
bert base has 12 layer and bert large has 24 layers  . in comparison to the bert with 6 layers  .
BERT worked on : 
Masked lanaguage models(MSMS)
In this there is a sentance givne with some fill in the blanck and we have to predict the work that fits in the most appripraite way .
This is also called the masking of the word . 
around `15% in a sentance the masking is done . 
if more masking is done then it becomes inapporpraite and less makes it expensive .
 
Next sdentance predication(NSP)
predict the whole next sentence after one given sentance . 
given A then predict B .


Training of BERT()
for the bert trainig we have to feed three different thing ->
1) tocken embedding 
2) segment embedding 
3) position embedding 
tocken embeddign are the basic sebntances that are fed into bert . 
example -> my dog is cute he likes playing .
segmant embeddign -> this is used to distinguish between the two token embedding recieved .
SEP is aspecial type of a segambnt embeddign that is used to seperate the two sentances  .SEP IS THE UNIT in bert  .

The input sequence in bert is 512 length .
The order of the input sequence is maintained by the segment called the postion embedding   .


Bert is used in the single process classficaitionlike the npl task -> natural langauge processing . 

